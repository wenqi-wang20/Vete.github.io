---
title: 深度神经网络中的梯度泄露问题
date: 2022-02-02 14:56:45
tags: 梯度泄露
categories: 深度学习
---

寒假摸鱼摸久了，才想起来年初是要找学姐交差的，要是再没有进展，估计明年的论文就没戏了。遂起身，爬下床，打开电脑开始看论文。

过年这几天虽然没少玩，但是也没少反思自己。觉得自己现在的成绩并不是完全没有原因的。尽管对外会说来到了顶级学府，身边多了更多了优秀的伙伴和竞争对手。但是我自己的内心知道，所谓鲶鱼效应，只有竞争才能激发自己的潜能。鄙人不才，但却自认为潜能远远没有开发完全。更多的时候，我因为个人性格的原因，会经常想得很多，喜欢漫无目的地乱走。但大家都知道，**如果以路灯为起点，漫无目的地乱走，最终的期望就是回到终点。**2021一整年对于我来说过得并不是很好。在这个同龄人在不断进步的时代，仿佛逆水行舟，不进则退。

**但是每个人都会经历这样一段经历吧，人生的极小值点**。所以对我来说，接下来的2022年，有一些确定的目标和计划就成为了迫在眉睫的事情。我相信自己的能力，从来都是如此。从科研方向来说，我还是得要在寒假期间，最起码需要了解梯度泄露是怎么回事，并且复现出代码。剩下的我想要看完吴恩达的机器学习课程，并且对于深度学习的花园书有所了解。争取在春季学期进行1-2个论文项目的参与。必须要在大三之前有所收获。

那么我们就进入正题。

### Abstract

首先作者告诉我们，在分布式训练或者协作训练中，梯度的共享是一件很正常的事。人们通常也很放心地认为梯度并不会泄露分布的训练数据。但是这篇文章用极其大胆的猜想和实验的佐证（包括$NLP$和图像处理的两个实验都得到了非常好的效果）告诉我们梯度共享也有可能会泄露用户数据，并且称之为$Deep \ Leakage \ from \ Gradients$。并且指出了以梯度剪枝$(gradients \ pruning)$为首的一些防御方法。

### Introduction

首先作者指出，已经有研究表明，梯度的共享会泄露一些$properties \ of \ training \ data$， 结合$GAN$就可以生成和原图像相似的图片。但是作者给出的***DLG***方法则声称可以还原完整的数据和标签，甚至不用借助任何其他的生成网络模型。

<img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpictures20220203225947.png" style="zoom:50%;" />

作者提出了一种优化算法来完成这一攻击：首先随机生成一种$dummy \ inputs$，然后通过正常的梯度推导过程生成一个$dummy \ gradients$，然后**这个算法并不会像一般的优化算法一样去优化模型的权重，而是会优化dummy gradients和正常的梯度之间的“距离”，然后修改dummy inputs和输入标签，使得我们的伪数据越来越接近真实的数据，最终优化算法结束的时候，就是伪数据和真实数据完全匹配的时候。**

同时，作者还指出**梯度扰动，低精度和梯度压缩三种方法中，尺度大于1e-2的高斯噪声和拉普拉斯噪声有明显的作用，20％的梯度剪枝同样也能起到保护的作用，但是半精度的方法却不起作用。**

### Related Work

首先注意到的是，与“深度梯度泄露”相对应的是，浅层的梯度泄露。前人的研究表示从梯度中推断训练数据的信息特征是可能的。例如在语言任务中对训练单词的梯度就可以揭露哪些单词参与了训练集，我们明显可以察觉到，这类泄露是**肤浅的**，并且也没有过多的作用。

然后作者指出了分布式机器学习中一定会出现的梯度交换问题。并且指出了无论是哪种分布方式，去中心化或者中心化的分布式训练，都有可能存在“深度梯度泄露”的问题。

<img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpictures20220203224347.png" style="zoom: 50%;" />

### Method

接下来我们来看看具体的方法。

首先我们关注传统的分布式学习方法，在第$t$轮训练过程中，第$i$方节点从训练集中选取$minibatch(x_{t,i},y_{t,i})$的数据来计算梯度

