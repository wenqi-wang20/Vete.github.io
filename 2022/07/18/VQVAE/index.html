
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>VQ-VAE - Drenched in my pain.</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="VAE
先让我们简单地看一下 VQGAN+CLIP 的效果。这是 2021
年开源的一套自然语言+视觉处理的结合实验，人类可以输入一些句子和短语，在经过分析之后，从
text 转为 image
的过,"> 
    <meta name="author" content="vinci"> 
    <link rel="alternative" href="atom.xml" title="Drenched in my pain." type="application/atom+xml"> 
    <link rel="icon" href="/img/my_favicon.png"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
<link rel="stylesheet" href="/css/diaspora.css">

    <link rel="stylesheet" href="/js/prism/prism.css">
<meta name="generator" content="Hexo 5.4.0"><link rel="stylesheet" href="/css/prism-hopscotch.css" type="text/css"></head>

<body class="loading">
    <span id="config-title" style="display:none">Drenched in my pain.</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">VQ-VAE</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">VQ-VAE</h1>
        <div class="stuff">
            <span>七月 18, 2022</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>


        </div>
        <div class="content markdown">
            <h2 id="vae">VAE</h2>
<p>先让我们简单地看一下 VQGAN+CLIP 的效果。这是 2021
年开源的一套自然语言+视觉处理的结合实验，人类可以输入一些句子和短语，在经过分析之后，从
text 转为 image
的过程。并且在合成图片的过程中，模型还会注意风格的融合。最新的研究成果是谷歌发布的
DALLE-2，风格非常炫酷，可惜没有开源，只有“内部人员”才能玩。下面是我在
Colab 上训练了 10 min 的结果，输入的 texts 是
<code>grown bears in the grass. |  a beautiful forest.</code></p>
<figure>
<img
src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesmovie.gif"
alt="movie" />
<figcaption aria-hidden="true">movie</figcaption>
</figure>
<p>现实中，很多输入的信息大量都是冗余无用的，我们可以使用某些压缩的、离散的表示就能够概括大部分内容。什么是
<code>AE</code>？自编码器，一般由三个部分构成：编码器、解码器以及中间隐藏层表示。我们可以将高维度的输入编码为低维度的表示，然后再通过解码器对该特征表示进行重建，<strong>训练的损失就是重建的输出与原始输入之间的误差。</strong>这样训练完成之后的编码器，可以结合
<code>few-shot learning</code>
进行特征学习，然后将学习后的特征用于分类任务；解码器则可以用于在特征空间上采样，生成新的输出。</p>
<p><img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesMDpicturesv2-ba9d7206d08e072b82401cec25415d2b_720w.jpg" alt="img" style="zoom: 67%;" /></p>
<p>而所谓
<code>VAE</code>，就是变分自编码器。为什么要产生变分自编码器？因为仅仅使用自编码器时，产生的隐式空间往往是离散的，如果我们从训练集没有覆盖的特征空间进行采样时，往往无法达到很好的效果，导致生成的图片
“四不像”。所以我们可以学习一个分布，编码器产生一个均值和另一个类似于
“标准差”
的输出，然后我们从正态分布中进行采样来给图片的特征向量加一个噪声。我们将这种扰动后的特征进行重建来生成输出。<strong>训练损失我们采用重建后的误差
+ 与混合高斯分布之间的 KL 散度两项</strong>。</p>
<p><img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesimage-20220713111254448.png" alt="image-20220713111254448" style="zoom: 25%;" /></p>
<p>混合模型 <code>GMM</code>，<span
class="math inline">\(P(x)=\sum_{m}P(m)P(x|m) \quad x|m \sim N(\mu(m),
\sigma(m))\)</span>。为了学习更加连续的分布，采用积分神经网路的方法，生成无限个高斯组件拟合真实分布，<span
class="math inline">\(P(x) = \int
P(m)P(x|m)\)</span>。我们采用一个向量<span class="math inline">\(z \sim
N(0, 1)\)</span>来生成分布。编码器把输入向量 <code>x</code>
映射到最可能的高斯分布 <code>z</code>
上，然后解码器将该分布生成网络。但是生成的图片质量往往较低。</p>
<p><img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesimage-20220713123751929.png" alt="image-20220713123751929" style="zoom:25%;" />
<span class="math display">\[
KL(N(\mu, \sigma), N(0, 1)) = \log \frac{1}{\sigma} + \frac{\sigma^2 +
\mu^2}{2} - \frac{1}{2}
\]</span></p>
<h2 id="neural-discrete-representation-learning">Neural Discrete
Representation Learning</h2>
<p><code>Aaron van den Oord Oriol Vinyals Koray Kavukcuoglu， NIPS'17</code></p>
<ul>
<li><code>the Vector Quantised Variational AutoEncoder (VQ-VAE)</code>矢量量化自动微分编码器</li>
<li><code>It can successfully model important features that usually span many dimensions in data space as opposed to focusing or spending capacity on noise and imperceptible details which are often local.</code>
成功让潜在空间离散化，能够更加充分地屏蔽噪声，更好地利用潜在空间。</li>
</ul>
<p>虽然说是 VAE，但其实模型建立过程中几乎没有涉及到变分的环节，看起来和
AE 倒是更像。</p>
<p><img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesimage-20220718102702579.png" alt="image-20220718102702579" style="zoom:60%;" /></p>
<h2
id="generating-diverse-high-fidelity-images-with-vq-vae-2">Generating
Diverse High-Fidelity Images with VQ-VAE-2</h2>
<p>VQ-VAE
的进阶版本，本质上并没有作出结构上的创新。主要针对经典版本作出了以下两点修改：</p>
<ul>
<li><p>对原始像素图片进行了分层编码，分层量化，并且最终根据分层的结果重建图像。这样的方法可以充分把握全局视野，也可以细化局部细节，生成更加逼真，细节更加清晰的图像。</p></li>
<li><p>在对离散编码的先验知识的拟合阶段，采用了 PixelCNN 网络结构。</p>
<p><img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesimage-20220718102402641.png" alt="image-20220718102402641" style="zoom: 67%;" ></p></li>
</ul>
<h2
id="vector-quantized-image-modeling-with-improved-vqgan">VECTOR-QUANTIZED
IMAGE MODELING WITH IMPROVED VQGAN</h2>
<p>这篇工作可以说是对 VQGAN
的改进，但其实我读的也不是特别明白，他到底改进了个啥……</p>
<ul>
<li>当然，第一个改进的地方就是把带有局部注意力模块的 CNN
编码器干脆直接换成了 Vision Transformer。</li>
<li>第二个改进是在 codebook 方面。这篇工作首先将 codebook
因子化，简单来说可能就是先从低维的空间寻找匹配的 latent code,
然后再映射到更高维度的 latent
code。这样做可以提高码本的使用率和重建图像的质量。</li>
<li>第三个小的 trick 就是文章将 latent code
进行了二范数归一化，所有向量的二范数都是一样大的，所以向量之间的欧氏距离就变成了类似余弦相似度，进一步提高了训练的稳定性。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesimage-20220718102746627.png" alt="image-20220718102746627" style="zoom:67%;" /></p>
<h2 id="maskgit-masked-generative-image-transformer">MaskGIT: Masked
Generative Image Transformer</h2>
<p>这一连着的四篇工作都是出自谷歌的实验室。这一篇其实也是将 NLP 中的
BERT
模型的优化方法带到了图像合成的任务中。作者认为，在做序列译码的时候，从左上到右下进行逐个译码是非常不自然的行为。首先将图像全部展平会导致图像序列非常长，很少有实际的自然语言任务需要这么长的序列。其次，人类画家在创造作品的时候，也并不是按照从左上到右下进行绘画的，他们一般都是先绘画出一部分的轮廓，再跳到另一部上色……等等，具有一定的随机性。所以这篇文章也是借用了这种思想。</p>
<ul>
<li>第一阶段训练 Encoder 和 Decoder 的设置和 VQGAN
相比没有太大的差别，作者沿用了之前的配置。</li>
<li>第二阶段训练先验分布的时候，文章采用了 MVTM 的方法，使用双向
Transformer 的机制并且设计掩码函数来帮助 Transformer
快速且并行地学习压缩后的 code matrix。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/wenqi-wang20/img/main/img/MDpicturesimage-20220718102820012.png" alt="image-20220718102820012" style="zoom: 60%;" /></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='true'
        data-ae='true'
        data-ci='ff344aadf12b27b1b89e'
        data-cs='f52347d027bfae073c0c4dcf016d54363b8f1957'
        data-r='wenqi-wang20.github.io'
        data-o='wenqi-wang20'
        data-a='wenqi-wang20'
        data-d='true'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>


</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
